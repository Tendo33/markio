<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>如何计算大型语言模型的 GPU 显存需求</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { font-family: "Helvetica Neue", Helvetica, Arial, "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei", sans-serif; margin: 0; padding: 0 1.5em; background: #fafbfc; color: #222; }
        .container { max-width: 700px; margin: 2em auto; background: #fff; border-radius: 8px; box-shadow: 0 2px 8px #eee; padding: 2em; }
        h1 { font-size: 2.2em; margin-bottom: 0.2em; }
        .meta { color: #888; font-size: 0.95em; margin-bottom: 1.5em; }
        audio { width: 100%; margin: 1em 0; }
        .content { line-height: 1.8; font-size: 1.1em; }
        hr { margin: 2em 0; border: none; border-top: 1px solid #eee; }
        .recommend { background: #f6f8fa; padding: 1em; border-radius: 6px; margin-top: 2em; }
    </style>
</head>
<body>
    <div class="container">
        <h1>如何计算大型语言模型的 GPU 显存需求</h1>
        <div class="meta">
            作者：<span>Wei Ming T.</span> | 日期：<span>2025-04-23</span>
        </div>
        <!-- 音频播放器，src替换为你的音频文件链接 -->
        <audio controls>
            <source src="your-podcast-audio.mp3" type="audio/mpeg">
            您的浏览器不支持音频播放。
        </audio>
        <div class="content">
            <p>运行大型语言模型（LLM）需要大量的计算资源，其中图形处理单元（GPU）的显存（VRAM）通常是最主要的瓶颈。显存不足会导致内存溢出（OOM）错误，阻碍推理或训练过程。相反，过度配置显存会导致不必要的成本和硬件资源的浪费。</p>
            <p>准确估算显存需求对于任何从事本地 LLM 开发的人员来说都至关重要。这种知识有助于做出明智的硬件选择，进行高效的资源管理，并成功部署或微调这些大规模模型。计算这些需求涉及考虑与模型架构、特定任务（推理或训练）以及所选配置相关的多个因素。</p>
            <p>本文是 VRAM 计算器的指南 ⬇️</p>
            <h2>为什么显存对 LLM 很重要</h2>
            <p>LLM 是庞大的神经网络，拥有数十亿个参数定义其学习的知识。在运行过程中，这些参数，以及中间计算结果（激活值），以及可能的梯度和优化器状态（在训练期间），必须存储在 GPU 的显存中以进行快速处理。</p>
            <p>可用的显存直接影响：</p>
            <ol>
                <li>可行性：确定特定大小和精度的模型是否可以加载到 GPU 上。</li>
                <li>性能：影响最大批量大小和序列长度，从而影响吞吐量和延迟。</li>
                <li>训练稳定性：训练期间的 OOM 错误可能会破坏进度或需要重新启动，浪费宝贵的时间和计算资源。</li>
            </ol>
            <h2>影响显存使用的组件</h2>
            <p>估算总显存涉及将多个不同组件消耗的内存相加。每个组件的重要性取决于是进行推理还是训练/微调。</p>
            <h3>模型参数</h3>
            <p>这通常是最大且最直接的组件。它由模型的参数数量和用于存储的数值精度决定。</p>
            <ul>
                <li>参数数量：通常以十亿为单位（例如，7B、70B、180B）。这通常可以在模型卡或其存储库中找到。</li>
                <li>精度（数据类型）：决定每个参数所需的字节数。
                    <ul>
                        <li>FP32（单精度浮点）：4 字节</li>
                        <li>FP16（半精度浮点）：2 字节</li>
                        <li>BF16（Bfloat16 浮点）：2 字节</li>
                        <li>INT8（8 位整数）：1 字节</li>
                        <li>INT4（4 位整数）：0.5 字节（打包）</li>
                    </ul>
                </li>
            </ul>
            <p>公式为：</p>
            <p>参数数量 × 每个参数的字节数 = 总字节数</p>
            <p>示例：一个 70 亿参数的模型（7B）以 FP16 精度加载需要：</p>
            <p>7 × 10⁹ 参数 × 2 字节/参数 = 14 × 10⁹ 字节 = 14 GB</p>
            <p>注意：此计算假设统一的精度，通常是最可预测的显存组件。</p>
            <h3>KV 缓存（推理生成）</h3>
            <p>在自回归生成过程中，模型缓存来自注意力层的过去的键（K）和值（V）状态，以加速令牌预测。此缓存随着生成的序列长度增长，并可能使用大量显存。其大小高度依赖于注意力机制结构和序列长度。</p>
            <p>模型的 KV 缓存大小的近似值通常表示为：</p>
            <p>2 × 层数 × 键/值头数 × 键/值投影的头维度 × 序列长度 × 批量大小 × 每个缓存值的字节数</p>
            <p>其中：</p>
            <ul>
                <li>每个缓存值的字节数：通常为 FP16/BF16 的 2 字节。</li>
                <li>键/值头数和键/值的头维度：这些取决于注意力架构（见下文的“注意力机制变体”）。</li>
            </ul>
            <p>KV 缓存量化：为了减少显存，KV 缓存可以被量化，例如，转换为 INT8 甚至 FP8（在支持的硬件上）。这将每个缓存值的字节数更改为 1（对于 INT8 或 FP8），显著减少 KV 缓存的显存使用。这可能会带来小的性能/准确性权衡，并需要框架支持。</p>
            <p>对于长序列或大批量，KV 缓存可能是推理显存使用的主要驱动因素。其大小估计也受实现细节影响。</p>
            <h3>注意力机制变体与显存</h3>
            <p>不同的注意力机制对显存有不同的影响，主要影响 KV 缓存大小和其他中间激活大小。设 N_q 为模型中的查询头数。</p>
            <ul>
                <li>多头注意力（MHA）：标准注意力，每个注意力头都有自己的查询、键和值投影。
                    <ul>
                        <li>在此配置中，键/值头数 N_kv = N_q。</li>
                    </ul>
                </li>
                <li>多查询注意力（MQA）：所有查询头共享一个键和值头。这大大减少了 KV 缓存的大小。
                    <ul>
                        <li>在此配置中，键/值头数 N_kv = 1。</li>
                        <li>KV 缓存显存变为 2 × 层数 × 1 × 键/值投影的头维度 × 序列长度 × 批量大小 × 每个缓存值的字节数。</li>
                    </ul>
                </li>
                <li>分组查询注意力（GQA）：查询头被分成若干组，每组共享一组键和值头。这在 MHA 和 MQA 之间提供了平衡。
                    <ul>
                        <li>键/值头数 N_kv 满足 1 < N_kv < N_q。</li>
                        <li>通常，N_kv = N_q / 组数。例如，如果 N_q = 32，且有 4 组，则 N_kv = 8。</li>
                    </ul>
                </li>
            </ul>
            <h2>推理的显存计算</h2>
            <p>对于推理，主要的显存贡献者是模型参数和激活（包括 KV 缓存）。</p>
            <p>总推理显存 ≈ 模型参数显存 + 激活显存 + KV 缓存显存 + 开销显存</p>
            <p>示例：Llama 3 8B（FP16）推理，GQA，序列长度 2048</p>
            <ol>
                <li>模型参数：8B 参数 × 2 字节/参数 = 16 GB</li>
                <li>激活和 KV 缓存：高度依赖于序列长度、批量大小和架构。对于批量大小为 4，序列长度为 2048：
                    <ul>
                        <li>Llama 3 8B 使用 GQA，具有 32 层，8 个键/值头 ...</li>
                    </ul>
                </li>
            </ol>
        </div>
        <hr>
        <div class="recommend">
            <strong>推荐阅读/收听：</strong>
            <ul>
                <li><a href="#">相关播客1</a></li>
                <li><a href="#">相关文章2</a></li>
            </ul>
        </div>
    </div>
</body>
</html>